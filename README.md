# ğŸ–¼ï¸ Image Captioning API (Backend)

A FastAPI-based backend service that generates captions for images using the BLIP model.

## ğŸš€ Features

- **Image Captioning**: Upload an image and get AI-generated descriptions
- **RESTful API**: Simple POST endpoint for image processing
- **BLIP Model**: Using Salesforce/blip-image-captioning-base

## ğŸ“ Project Structure

```
backend/
â”œâ”€â”€ app.py          # FastAPI application & API endpoints
â”œâ”€â”€ inference.py    # Core ML logic (BLIP model integration)
â”œâ”€â”€ eval.py         # Performance evaluation script
â”œâ”€â”€ requirement.txt # Python dependencies
â””â”€â”€ README.md       # This file
```

## ğŸ› ï¸ Installation

```bash
# Clone the repository
git clone https://github.com/feza-gulbuz/mis453-backend.git
cd mis453-backend

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirement.txt
```

## â–¶ï¸ Running the Server

```bash
uvicorn app:app --host 0.0.0.0 --port 8000
```

The API will be available at `http://localhost:8000`

## ğŸ“¡ API Endpoints

### GET `/`
Health check endpoint.

**Response:**
```json
{"message": "Image Captioning API - POST /caption endpoint'ine resim yÃ¼kleyin"}
```

### POST `/caption`
Upload an image to get a caption.

**Request:** `multipart/form-data` with `file` field

**Response:**
```json
{
  "caption": "a woman wearing a red dress",
  "filename": "image.jpg"
}
```

## ğŸ§ª Testing

```bash
# Run evaluation script
python eval.py

# Test inference directly
python inference.py
```

## ğŸ“¦ Dependencies

- FastAPI
- Uvicorn
- Transformers (BLIP model)
- Pillow
- PyTorch

## ğŸ“„ License

MIT License
